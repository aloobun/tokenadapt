# debug_heuristics.py
import torch
import faiss
import numpy as np
import time
import argparse
import random
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
from tqdm.auto import tqdm
import torch.nn.functional as F
import gc

# Import potentially modified heuristic functions
from heuristics import calculate_local_embedding, calculate_global_embedding
# Assuming cache functions are in cache.py (or adapt as needed)
from cache import load_cache, cache_embeddings # Need caching for aux embeds

# --- Helper Functions ---

def load_components(args):
    """Loads models, tokenizers, and FAISS index."""
    print("Loading components...")
    dtype_map = {"bf16": torch.bfloat16, "fp16": torch.float16, "fp32": torch.float32}
    dtype = dtype_map[args.dtype]
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}, dtype: {args.dtype}")

    # Load main model (on CPU initially to save VRAM)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_path, torch_dtype=dtype, device_map="cpu", token=args.hf_token
    )
    # Get original embeddings
    original_input_embeddings = model.get_input_embeddings().weight.clone().detach()
    original_output_embeddings = None
    tied = getattr(model.config, "tie_word_embeddings", False)
    if not tied:
        output_layer = model.get_output_embeddings()
        if output_layer is not None:
            original_output_embeddings = output_layer.weight.clone().detach()
            if output_layer.weight is model.get_input_embeddings().weight:
                tied = True # Double check if they point to the same tensor
            else:
                 tied = False
        else: # No output layer means tied? Or error? Assume tied if None.
            tied = True
    print(f"Model embeddings tied: {tied}")


    # Load tokenizers
    old_tokenizer = AutoTokenizer.from_pretrained(args.model_path, token=args.hf_token)
    new_tokenizer = AutoTokenizer.from_pretrained(args.new_tokenizer_path, token=args.hf_token)
    old_vocab = old_tokenizer.get_vocab()
    new_vocab = new_tokenizer.get_vocab()

    # Load aux embedding model (can place on GPU)
    print("Loading auxiliary embedding model...")
    embed_model = AutoModel.from_pretrained(args.embedding_model_path, trust_remote_code=True).to(device)
    embed_tokenizer = AutoTokenizer.from_pretrained(args.embedding_model_path, trust_remote_code=True)
    try:
        embed_dim_external = embed_model.config.hidden_size
    except AttributeError:
        # Try to infer later if needed
         embed_dim_external = None
    print(f"Aux embedding model loaded. External dim: {embed_dim_external}")

    # Vocab processing
    shared_vocab = list(set(new_vocab.keys()) & set(old_vocab.keys()))
    unique_tokens_set = set(new_vocab.keys()) - set(shared_vocab)
    print(f"Total unique tokens: {len(unique_tokens_set)}")

    # --- Caching Auxiliary Embeddings ---
    # Load or create cache for aux embeddings (essential for performance)
    embed_model_name = args.embedding_model_path.split("/")[-1]
    cache_file = f"cache_{embed_model_name}_debug.json" # Use separate cache if desired
    aux_cache = load_cache(cache_file)
    print(f"Loaded {len(aux_cache)} items from aux cache.")

    # Tokens needed for local heuristic (full new tokens + old subtokens)
    tokens_to_cache_local = set()
    # 1. Full decoded unique tokens
    unique_token_strings = list(unique_tokens_set) # Use list for deterministic iteration
    random.shuffle(unique_token_strings) # Shuffle if needed
    sample_unique_tokens = unique_token_strings[:args.num_samples]

    decoded_sample = [new_tokenizer.decode([new_vocab[t]]) for t in sample_unique_tokens if t in new_vocab]
    tokens_to_cache_local.update(t for t in decoded_sample if t not in aux_cache)

    # 2. Subtokens generated by old_tokenizer for the sample
    subtokens_needed = set()
    print("Identifying required subtokens...")
    for full_decoded in tqdm(decoded_sample, desc="Encoding with old tokenizer"):
        old_ids = old_tokenizer.encode(full_decoded, add_special_tokens=False)
        for oid in old_ids:
            sub = old_tokenizer.decode([oid])
            if sub not in aux_cache:
                 subtokens_needed.add(sub)
    tokens_to_cache_local.update(subtokens_needed)

    # Tokens needed for global heuristic (full new tokens + all old tokens for index)
    tokens_to_cache_global = set(decoded_sample) # Already added above
    # 3. All old vocabulary tokens (needed for FAISS index)
    old_tokens_for_index = set()
    print("Identifying old vocab tokens for index...")
    for t, token_id in tqdm(old_vocab.items(), desc="Decoding old vocab"):
        decoded_old = old_tokenizer.decode([token_id])
        if decoded_old not in aux_cache:
            old_tokens_for_index.add(decoded_old)
    tokens_to_cache_global.update(old_tokens_for_index)

    # Cache everything needed
    all_tokens_to_cache = list(tokens_to_cache_local.union(tokens_to_cache_global))
    if all_tokens_to_cache:
        print(f"Caching {len(all_tokens_to_cache)} auxiliary embeddings...")
        aux_cache = cache_embeddings(embed_model, embed_tokenizer, all_tokens_to_cache, device,
                                     aux_cache, batch_size=args.batch_size)
        # save_cache(cache_file, aux_cache) # Optionally save cache
    else:
        print("All necessary auxiliary embeddings seem to be cached.")

    # Separate caches for heuristics
    full_token_embeds_cache = {t: aux_cache[t] for t in decoded_sample if t in aux_cache}
    subtoken_embeds_cache = {t: aux_cache[t] for t in subtokens_needed.union(set(aux_cache.keys())) if t in aux_cache} # Be comprehensive

    # Prepare data for FAISS index
    old_vocab_embeds_for_index = {}
    print("Preparing FAISS index data...")
    for token, oid in tqdm(old_vocab.items(), desc="Gathering old embeds for FAISS"):
         decoded_old = old_tokenizer.decode([oid])
         if decoded_old in aux_cache:
             old_vocab_embeds_for_index[token] = aux_cache[decoded_old] # Map original token string to embed

    # --- Build FAISS Index ---
    if not embed_dim_external: # Try to infer if not set
        first_embed = next(iter(old_vocab_embeds_for_index.values()), None)
        if first_embed:
            embed_dim_external = len(first_embed)
            print(f"Inferred external dimension: {embed_dim_external}")
        else:
            raise ValueError("Cannot determine external embedding dimension.")

    print("Building FAISS index...")
    faiss_index = None
    index_to_token = None
    if old_vocab_embeds_for_index:
        token_list_faiss = []
        embedding_matrix_list = []
        for token, embed_list in old_vocab_embeds_for_index.items():
             try:
                 embed_np = np.array(embed_list, dtype=np.float32)
                 if embed_np.shape == (embed_dim_external,):
                     token_list_faiss.append(token)
                     embedding_matrix_list.append(embed_np)
             except Exception as e:
                 print(f"Skipping token '{token}' for FAISS due to error: {e}")

        if embedding_matrix_list:
            embedding_matrix = np.vstack(embedding_matrix_list)
            faiss.normalize_L2(embedding_matrix)
            faiss_index = faiss.IndexFlatIP(embed_dim_external)
            faiss_index.add(embedding_matrix)
            index_to_token = {i: token for i, token in enumerate(token_list_faiss)}
            print(f"FAISS index built with {faiss_index.ntotal} vectors.")
        else:
             print("Warning: No valid vectors to build FAISS index.")
    else:
        print("Warning: No embeddings found for old vocab to build FAISS index.")


    # Free up GPU memory from aux model if needed
    del embed_model
    gc.collect()
    torch.cuda.empty_cache()

    return {
        "original_input_embeddings": original_input_embeddings,
        "original_output_embeddings": original_output_embeddings,
        "old_tokenizer": old_tokenizer,
        "new_tokenizer": new_tokenizer,
        "old_vocab": old_vocab,
        "new_vocab": new_vocab,
        "full_token_embeds_cache": full_token_embeds_cache,
        "subtoken_embeds_cache": subtoken_embeds_cache,
        "faiss_index": faiss_index,
        "index_to_token": index_to_token,
        "sample_unique_tokens": sample_unique_tokens, # The actual token strings to test
        "data_type": dtype,
        "calc_device": torch.device(device if torch.cuda.is_available() else "cpu") # Device for heuristic calcs
    }


def main(args):
    """Runs the heuristic comparison."""
    components = load_components(args)
    if not components['sample_unique_tokens']:
        print("No unique tokens selected or found. Exiting.")
        return
    if components['faiss_index'] is None:
        print("Warning: FAISS index not available. Global heuristic cannot be tested.")

    print(f"\n--- Comparing Heuristics for {len(components['sample_unique_tokens'])} Unique Tokens ---")

    results = []
    local_success = 0
    global_success = 0
    both_success = 0

    # Move original embeddings to calculation device
    calc_device = components['calc_device']
    print(f"Using device for calculations: {calc_device}")
    orig_input_embeds = components['original_input_embeddings'].to(calc_device, dtype=components['data_type'])
    orig_output_embeds = None
    if components['original_output_embeddings'] is not None:
        orig_output_embeds = components['original_output_embeddings'].to(calc_device, dtype=components['data_type'])


    for token_str in tqdm(components['sample_unique_tokens'], desc="Analyzing Tokens"):
        new_id = components['new_vocab'].get(token_str)
        if new_id is None:
            print(f"Warning: Token '{token_str}' not found in new vocab.")
            continue

        # --- Run Local Heuristic ---
        e_local_in, e_local_out, details_local = calculate_local_embedding(
            token_str=token_str,
            new_token_id=new_id,
            new_tokenizer=components['new_tokenizer'],
            old_tokenizer=components['old_tokenizer'],
            full_token_embeds_cache=components['full_token_embeds_cache'],
            subtoken_embeds_cache=components['subtoken_embeds_cache'],
            original_input_embeddings=orig_input_embeds,
            original_output_embeddings=orig_output_embeds,
            temperature=args.temperature,
            data_type=components['data_type'],
            device=calc_device,
            return_details=True
        )

        # --- Run Global Heuristic ---
        e_global_in, e_global_out, details_global = None, None, None
        if components['faiss_index'] is not None and components['index_to_token'] is not None:
            # Need the *decoded* string for global cache lookup
            full_decoded_str = components['new_tokenizer'].decode([new_id])
            if full_decoded_str in components['full_token_embeds_cache']:
                e_global_in, e_global_out, details_global = calculate_global_embedding(
                    query_token_str=full_decoded_str, # Use decoded string
                    full_token_embeds_cache=components['full_token_embeds_cache'],
                    faiss_index=components['faiss_index'],
                    old_tokenizer=components['old_tokenizer'],
                    index_to_token=components['index_to_token'],
                    old_vocab=components['old_vocab'],
                    original_input_embeddings=orig_input_embeds,
                    original_output_embeddings=orig_output_embeds,
                    k=args.top_k,
                    temperature=args.temperature,
                    data_type=components['data_type'],
                    device=calc_device,
                    return_details=True
                )
            # else: print(f"Decoded '{full_decoded_str}' not in cache for global.")
        # else: print("Skipping global due to missing FAISS index.")

        # --- Compare Results ---
        token_result = {"token": token_str, "id": new_id}
        local_ok = e_local_in is not None
        global_ok = e_global_in is not None

        if local_ok: local_success += 1
        if global_ok: global_success += 1
        if local_ok and global_ok: both_success += 1

        token_result['local_success'] = local_ok
        token_result['global_success'] = global_ok
        token_result['local_details'] = details_local
        token_result['global_details'] = details_global

        if local_ok and global_ok:
            # Ensure tensors are on CPU for numpy conversion if needed, and correct type
            e_local_in_comp = e_local_in.to(torch.float32) # Use float32 for comparison
            e_global_in_comp = e_global_in.to(torch.float32)

            # Cosine Similarity
            cos_sim = F.cosine_similarity(e_local_in_comp.unsqueeze(0), e_global_in_comp.unsqueeze(0)).item()
            token_result['cosine_similarity'] = cos_sim

            # L2 Distance
            l2_dist = torch.linalg.norm(e_local_in_comp - e_global_in_comp).item()
            token_result['l2_distance'] = l2_dist

            # Norms
            token_result['local_norm'] = torch.linalg.norm(e_local_in_comp).item()
            token_result['global_norm'] = torch.linalg.norm(e_global_in_comp).item()

        results.append(token_result)

    # --- Print Summary and Detailed Analysis ---
    print("\n--- Analysis Summary ---")
    total_analyzed = len(components['sample_unique_tokens'])
    print(f"Tokens Analyzed: {total_analyzed}")
    print(f"Local Success: {local_success} ({local_success/total_analyzed:.2%})")
    print(f"Global Success: {global_success} ({global_success/total_analyzed:.2%})")
    print(f"Both Success: {both_success} ({both_success/total_analyzed:.2%})")

    successful_comparisons = [r for r in results if r['local_success'] and r['global_success']]
    if successful_comparisons:
        avg_cos_sim = np.mean([r['cosine_similarity'] for r in successful_comparisons])
        avg_l2_dist = np.mean([r['l2_distance'] for r in successful_comparisons])
        avg_local_norm = np.mean([r['local_norm'] for r in successful_comparisons])
        avg_global_norm = np.mean([r['global_norm'] for r in successful_comparisons])
        print(f"\nMetrics for {len(successful_comparisons)} tokens where both succeeded:")
        print(f"  Avg. Cosine Similarity: {avg_cos_sim:.4f}")
        print(f"  Avg. L2 Distance:       {avg_l2_dist:.4f}")
        print(f"  Avg. Local Norm:        {avg_local_norm:.4f}")
        print(f"  Avg. Global Norm:       {avg_global_norm:.4f}")

    print("\n--- Detailed Token Analysis (Sample) ---")
    for i, res in enumerate(results[:args.detail_limit]): # Limit detailed output
        print(f"\nToken {i+1}: '{res['token']}' (ID: {res['id']})")
        print(f"  Local Success: {res['local_success']}, Global Success: {res['global_success']}")
        if res['local_success'] and res['global_success']:
            print(f"  Cosine Sim: {res['cosine_similarity']:.4f}, L2 Dist: {res['l2_distance']:.4f}")
            print(f"  Norms (Local/Global): {res['local_norm']:.4f} / {res['global_norm']:.4f}")

        if args.show_details:
             if res['local_details']:
                 print("  Local Details:")
                 details = res['local_details']
                 contributors = sorted(zip(details['weights'], details['contributor_tokens']), reverse=True)
                 print(f"    Top Contributors: {contributors[:5]}")
                 # print(f"    Weights: {details['weights']}") # Can be long
             if res['global_details']:
                 print("  Global Details:")
                 details = res['global_details']
                 contributors = sorted(zip(details['weights'], details['contributor_tokens']), reverse=True)
                 print(f"    Top Contributors (Neighbors): {contributors[:5]}")
                 # print(f"    Weights: {details['weights']}") # Can be long


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Debug and Compare Local/Global Heuristics")
    # Add arguments similar to transplant.py
    parser.add_argument("-model", "--model_path", required=True)
    parser.add_argument("-tk", "--new_tokenizer_path", required=True)
    parser.add_argument("-embed", "--embedding_model_path", default="nomic-ai/nomic-embed-text-v2-moe")
    parser.add_argument("-auth", "--hf_token", required=True)
    parser.add_argument("-temp", "--temperature", default=0.3, type=float)
    parser.add_argument("-k", "--top_k", default=3, type=int, help="K for Global heuristic")
    parser.add_argument("-d", "--dtype", default="fp32", choices=["bf16", "fp16", "fp32"])
    parser.add_argument("-bs", "--batch_size", default=16, type=int, help="Batch size for aux embedding caching")
    parser.add_argument("-n", "--num_samples", default=100, type=int, help="Number of unique tokens to analyze")
    parser.add_argument("--detail_limit", default=10, type=int, help="Max number of tokens for detailed printout")
    parser.add_argument("--show_details", action='store_true', help="Show detailed contributor lists")
    # Add other necessary args if needed

    args = parser.parse_args()
    main(args)
